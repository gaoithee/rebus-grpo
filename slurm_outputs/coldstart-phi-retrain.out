---------------------------------------------
SLURM job ID:        15350
SLURM job node list: lovelace-02
DATE:                Fri May 30 11:19:18 AM CEST 2025
---------------------------------------------
no change     /opt/spack/opt/spack/linux-rocky9-x86_64/gcc-13.2.0/miniconda3-22.11.1-tn534fvb4uy4wrf7m2zcwpiycdzlebd6/condabin/conda
no change     /opt/spack/opt/spack/linux-rocky9-x86_64/gcc-13.2.0/miniconda3-22.11.1-tn534fvb4uy4wrf7m2zcwpiycdzlebd6/bin/conda
no change     /opt/spack/opt/spack/linux-rocky9-x86_64/gcc-13.2.0/miniconda3-22.11.1-tn534fvb4uy4wrf7m2zcwpiycdzlebd6/bin/conda-env
no change     /opt/spack/opt/spack/linux-rocky9-x86_64/gcc-13.2.0/miniconda3-22.11.1-tn534fvb4uy4wrf7m2zcwpiycdzlebd6/bin/activate
no change     /opt/spack/opt/spack/linux-rocky9-x86_64/gcc-13.2.0/miniconda3-22.11.1-tn534fvb4uy4wrf7m2zcwpiycdzlebd6/bin/deactivate
no change     /opt/spack/opt/spack/linux-rocky9-x86_64/gcc-13.2.0/miniconda3-22.11.1-tn534fvb4uy4wrf7m2zcwpiycdzlebd6/etc/profile.d/conda.sh
no change     /opt/spack/opt/spack/linux-rocky9-x86_64/gcc-13.2.0/miniconda3-22.11.1-tn534fvb4uy4wrf7m2zcwpiycdzlebd6/etc/fish/conf.d/conda.fish
no change     /opt/spack/opt/spack/linux-rocky9-x86_64/gcc-13.2.0/miniconda3-22.11.1-tn534fvb4uy4wrf7m2zcwpiycdzlebd6/shell/condabin/Conda.psm1
no change     /opt/spack/opt/spack/linux-rocky9-x86_64/gcc-13.2.0/miniconda3-22.11.1-tn534fvb4uy4wrf7m2zcwpiycdzlebd6/shell/condabin/conda-hook.ps1
no change     /opt/spack/opt/spack/linux-rocky9-x86_64/gcc-13.2.0/miniconda3-22.11.1-tn534fvb4uy4wrf7m2zcwpiycdzlebd6/lib/python3.10/site-packages/xontrib/conda.xsh
no change     /opt/spack/opt/spack/linux-rocky9-x86_64/gcc-13.2.0/miniconda3-22.11.1-tn534fvb4uy4wrf7m2zcwpiycdzlebd6/etc/profile.d/conda.csh
no change     /u/scandussio/.bashrc
No action taken.
ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!
INFO 05-30 11:19:37 __init__.py:190] Automatically detected platform cuda.
==((====))==  Unsloth 2025.5.6: Fast Mistral patching. Transformers: 4.51.3. vLLM: 0.7.2.
   \\   /|    NVIDIA A100 80GB PCIe MIG 1g.20gb. Num GPUs = 1. Max memory: 19.5 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.5.1+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.1.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 0.4191, 'grad_norm': 0.15330110490322113, 'learning_rate': 0.0001755, 'epoch': 0.06}
{'loss': 0.0964, 'grad_norm': 0.15159177780151367, 'learning_rate': 0.00010049999999999999, 'epoch': 0.12}
{'loss': 0.0682, 'grad_norm': 0.25041693449020386, 'learning_rate': 2.5500000000000003e-05, 'epoch': 0.18}
{'eval_loss': 0.08238275349140167, 'eval_runtime': 236.5146, 'eval_samples_per_second': 8.448, 'eval_steps_per_second': 1.057, 'epoch': 0.2}
{'train_runtime': 5053.9428, 'train_samples_per_second': 3.166, 'train_steps_per_second': 0.099, 'train_loss': 0.1815305576324463, 'epoch': 0.2}
Unsloth: Merging 4bit and LoRA weights to 16bit...
Unsloth: Will use up to 513.1 out of 754.96 RAM for saving.
Unsloth: Saving model... This might take 5 minutes ...
Unsloth: Saving to organization with address saracandu/phi3-mini-rebus-solver-coldstart-retrained
Unsloth: Saving tokenizer... Done.
Unsloth: Saving to organization with address saracandu/phi3-mini-rebus-solver-coldstart-retrained
Unsloth: Uploading all files... Please wait...
Done.
Saved merged model to https://huggingface.co/None/phi3-mini-rebus-solver-coldstart-retrained
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33moutputs[0m at: [34mhttps://wandb.ai/saracandussio-universit-degli-studi-di-trieste/huggingface/runs/htwo15wa[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250530_111954-htwo15wa/logs[0m
DONE!
