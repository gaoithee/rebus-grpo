---------------------------------------------
SLURM job ID:        14703
SLURM job node list: lovelace-02
DATE:                Wed May 21 03:59:30 PM CEST 2025
---------------------------------------------
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.
ü¶• Unsloth Zoo will now patch everything to make training faster!
INFO 05-21 15:59:46 __init__.py:190] Automatically detected platform cuda.
wandb: Appending key for api.wandb.ai to your netrc file: /u/scandussio/.netrc
wandb: Currently logged in as: saracandussio (saracandussio-universit-degli-studi-di-trieste) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
Unsloth 2025.5.6 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /share/ai-lab/scandussio/rebus-grpo/wandb/run-20250521_160000-09z85ccl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fast-darkness-21
wandb: ‚≠êÔ∏è View project at https://wandb.ai/saracandussio-universit-degli-studi-di-trieste/phi-GRPO
wandb: üöÄ View run at https://wandb.ai/saracandussio-universit-degli-studi-di-trieste/phi-GRPO/runs/09z85ccl
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 13,500 | Num Epochs = 1 | Total steps = 750
O^O/ \_/ \    Batch size per device = 12 | Gradient accumulation steps = 6
\        /    Data Parallel GPUs = 1 | Total batch size (12 x 6 x 1) = 72
 "-____-"     Trainable parameters = 29,884,416/4,000,000,000 (0.75% trained)
wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
==((====))==  Unsloth 2025.5.6: Fast Mistral patching. Transformers: 4.51.3. vLLM: 0.7.2.
   \\   /|    NVIDIA A100 80GB PCIe. Num GPUs = 1. Max memory: 79.138 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.5.1+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.1.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Training begins...
  0%|          | 0/750 [00:00<?, ?it/s]  0%|          | 1/750 [02:35<32:17:31, 155.21s/it]  0%|          | 2/750 [05:05<31:37:22, 152.20s/it]  0%|          | 3/750 [07:34<31:16:32, 150.73s/it]  1%|          | 4/750 [10:03<31:06:01, 150.08s/it]  1%|          | 5/750 [12:32<30:57:32, 149.60s/it]  1%|          | 6/750 [15:01<30:52:49, 149.42s/it]  1%|          | 7/750 [17:30<30:47:59, 149.23s/it]