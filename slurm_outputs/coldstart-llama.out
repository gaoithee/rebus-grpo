---------------------------------------------
SLURM job ID:        15315
SLURM job node list: babbage
DATE:                Thu May 29 04:50:53 PM CEST 2025
---------------------------------------------
no change     /opt/spack/opt/spack/linux-rocky9-x86_64/gcc-13.2.0/miniconda3-22.11.1-tn534fvb4uy4wrf7m2zcwpiycdzlebd6/condabin/conda
no change     /opt/spack/opt/spack/linux-rocky9-x86_64/gcc-13.2.0/miniconda3-22.11.1-tn534fvb4uy4wrf7m2zcwpiycdzlebd6/bin/conda
no change     /opt/spack/opt/spack/linux-rocky9-x86_64/gcc-13.2.0/miniconda3-22.11.1-tn534fvb4uy4wrf7m2zcwpiycdzlebd6/bin/conda-env
no change     /opt/spack/opt/spack/linux-rocky9-x86_64/gcc-13.2.0/miniconda3-22.11.1-tn534fvb4uy4wrf7m2zcwpiycdzlebd6/bin/activate
no change     /opt/spack/opt/spack/linux-rocky9-x86_64/gcc-13.2.0/miniconda3-22.11.1-tn534fvb4uy4wrf7m2zcwpiycdzlebd6/bin/deactivate
no change     /opt/spack/opt/spack/linux-rocky9-x86_64/gcc-13.2.0/miniconda3-22.11.1-tn534fvb4uy4wrf7m2zcwpiycdzlebd6/etc/profile.d/conda.sh
no change     /opt/spack/opt/spack/linux-rocky9-x86_64/gcc-13.2.0/miniconda3-22.11.1-tn534fvb4uy4wrf7m2zcwpiycdzlebd6/etc/fish/conf.d/conda.fish
no change     /opt/spack/opt/spack/linux-rocky9-x86_64/gcc-13.2.0/miniconda3-22.11.1-tn534fvb4uy4wrf7m2zcwpiycdzlebd6/shell/condabin/Conda.psm1
no change     /opt/spack/opt/spack/linux-rocky9-x86_64/gcc-13.2.0/miniconda3-22.11.1-tn534fvb4uy4wrf7m2zcwpiycdzlebd6/shell/condabin/conda-hook.ps1
no change     /opt/spack/opt/spack/linux-rocky9-x86_64/gcc-13.2.0/miniconda3-22.11.1-tn534fvb4uy4wrf7m2zcwpiycdzlebd6/lib/python3.10/site-packages/xontrib/conda.xsh
no change     /opt/spack/opt/spack/linux-rocky9-x86_64/gcc-13.2.0/miniconda3-22.11.1-tn534fvb4uy4wrf7m2zcwpiycdzlebd6/etc/profile.d/conda.csh
no change     /u/scandussio/.bashrc
No action taken.
ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!
INFO 05-29 16:51:24 __init__.py:190] Automatically detected platform cuda.
==((====))==  Unsloth 2025.5.6: Fast Llama patching. Transformers: 4.51.3. vLLM: 0.7.2.
   \\   /|    NVIDIA A100-PCIE-40GB. Num GPUs = 1. Max memory: 39.381 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.5.1+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.1.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Dataset({
    features: ['system-prompt', 'prompt', 'answer', 'text'],
    num_rows: 180
})
Training starts
{'loss': 1.3703, 'grad_norm': 0.15181420743465424, 'learning_rate': 0.00019564245810055868, 'epoch': 0.28}
{'loss': 0.0441, 'grad_norm': 0.17322003841400146, 'learning_rate': 0.00019005586592178773, 'epoch': 0.56}
{'loss': 0.0435, 'grad_norm': 0.013746698386967182, 'learning_rate': 0.00018446927374301676, 'epoch': 0.83}
{'loss': 0.0436, 'grad_norm': 0.015275908634066582, 'learning_rate': 0.00017888268156424582, 'epoch': 1.11}
{'loss': 0.0435, 'grad_norm': 0.18401053547859192, 'learning_rate': 0.00017329608938547485, 'epoch': 1.39}
{'loss': 0.0435, 'grad_norm': 0.18756790459156036, 'learning_rate': 0.0001677094972067039, 'epoch': 1.67}
{'loss': 0.0436, 'grad_norm': 0.1805478036403656, 'learning_rate': 0.00016212290502793297, 'epoch': 1.94}
{'loss': 0.0436, 'grad_norm': 0.013947848230600357, 'learning_rate': 0.000156536312849162, 'epoch': 2.22}
{'loss': 0.0435, 'grad_norm': 0.36674800515174866, 'learning_rate': 0.00015094972067039106, 'epoch': 2.5}
{'loss': 0.0435, 'grad_norm': 0.007332985755056143, 'learning_rate': 0.00014536312849162012, 'epoch': 2.78}
{'loss': 0.0436, 'grad_norm': 0.37227001786231995, 'learning_rate': 0.00013977653631284918, 'epoch': 3.06}
{'loss': 0.0435, 'grad_norm': 0.006934179458767176, 'learning_rate': 0.0001341899441340782, 'epoch': 3.33}
{'loss': 0.0435, 'grad_norm': 0.18515850603580475, 'learning_rate': 0.00012860335195530727, 'epoch': 3.61}
{'loss': 0.0437, 'grad_norm': 0.2022000402212143, 'learning_rate': 0.00012301675977653633, 'epoch': 3.89}
{'loss': 0.0435, 'grad_norm': 0.01260615885257721, 'learning_rate': 0.00011743016759776537, 'epoch': 4.17}
{'loss': 0.0435, 'grad_norm': 0.22887445986270905, 'learning_rate': 0.00011184357541899441, 'epoch': 4.44}
{'loss': 0.0435, 'grad_norm': 0.003119493369013071, 'learning_rate': 0.00010625698324022346, 'epoch': 4.72}
{'loss': 0.0436, 'grad_norm': 0.01347118616104126, 'learning_rate': 0.00010067039106145253, 'epoch': 5.0}
{'loss': 0.0435, 'grad_norm': 0.2080851048231125, 'learning_rate': 9.508379888268158e-05, 'epoch': 5.28}
{'loss': 0.0436, 'grad_norm': 0.0062914262525737286, 'learning_rate': 8.949720670391062e-05, 'epoch': 5.56}
{'loss': 0.0436, 'grad_norm': 0.006552162114530802, 'learning_rate': 8.391061452513967e-05, 'epoch': 5.83}
{'loss': 0.0435, 'grad_norm': 0.003880836768075824, 'learning_rate': 7.832402234636872e-05, 'epoch': 6.11}
{'loss': 0.0436, 'grad_norm': 0.006086672656238079, 'learning_rate': 7.273743016759777e-05, 'epoch': 6.39}
{'loss': 0.0435, 'grad_norm': 0.23320208489894867, 'learning_rate': 6.715083798882681e-05, 'epoch': 6.67}
{'loss': 0.0436, 'grad_norm': 0.1837090253829956, 'learning_rate': 6.156424581005586e-05, 'epoch': 6.94}
{'loss': 0.0435, 'grad_norm': 0.004696756601333618, 'learning_rate': 5.5977653631284924e-05, 'epoch': 7.22}
{'loss': 0.0435, 'grad_norm': 0.18367356061935425, 'learning_rate': 5.039106145251397e-05, 'epoch': 7.5}
{'loss': 0.0435, 'grad_norm': 0.003638763213530183, 'learning_rate': 4.480446927374302e-05, 'epoch': 7.78}
{'loss': 0.0435, 'grad_norm': 0.18918390572071075, 'learning_rate': 3.9217877094972065e-05, 'epoch': 8.06}
{'loss': 0.0435, 'grad_norm': 0.1669723242521286, 'learning_rate': 3.3631284916201116e-05, 'epoch': 8.33}
{'loss': 0.0435, 'grad_norm': 0.006746563129127026, 'learning_rate': 2.8044692737430168e-05, 'epoch': 8.61}
{'loss': 0.0435, 'grad_norm': 0.004963456653058529, 'learning_rate': 2.245810055865922e-05, 'epoch': 8.89}
{'loss': 0.0434, 'grad_norm': 0.1786656379699707, 'learning_rate': 1.6871508379888268e-05, 'epoch': 9.17}
{'loss': 0.0435, 'grad_norm': 0.006054324563592672, 'learning_rate': 1.1284916201117319e-05, 'epoch': 9.44}
{'loss': 0.0435, 'grad_norm': 0.1795673370361328, 'learning_rate': 5.698324022346369e-06, 'epoch': 9.72}
{'loss': 0.0434, 'grad_norm': 0.0035646117758005857, 'learning_rate': 1.1173184357541899e-07, 'epoch': 10.0}
{'train_runtime': 1185.9122, 'train_samples_per_second': 1.518, 'train_steps_per_second': 1.518, 'train_loss': 0.08039049598905776, 'epoch': 10.0}
Training ends
Unsloth: Merging 4bit and LoRA weights to 16bit...
Unsloth: Will use up to 549.79 out of 754.01 RAM for saving.
Unsloth: Saving model... This might take 5 minutes ...
Unsloth: Saving to organization with address saracandu/llama-3.1-8b-rebus-solver-coldstart
Unsloth: Saving tokenizer... Done.
Unsloth: Saving to organization with address saracandu/llama-3.1-8b-rebus-solver-coldstart
Unsloth: Uploading all files... Please wait...
Done.
Saved merged model to https://huggingface.co/None/llama-3.1-8b-rebus-solver-coldstart
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mcolorful-thunder-3[0m at: [34mhttps://wandb.ai/saracandussio-universit-degli-studi-di-trieste/cold-start-llama/runs/feq8upw4[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250529_165152-feq8upw4/logs[0m
DONE!
