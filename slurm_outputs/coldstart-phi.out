---------------------------------------------
SLURM job ID:        15317
SLURM job node list: lovelace-02
DATE:                Thu May 29 04:54:50 PM CEST 2025
---------------------------------------------
no change     /opt/spack/opt/spack/linux-rocky9-x86_64/gcc-13.2.0/miniconda3-22.11.1-tn534fvb4uy4wrf7m2zcwpiycdzlebd6/condabin/conda
no change     /opt/spack/opt/spack/linux-rocky9-x86_64/gcc-13.2.0/miniconda3-22.11.1-tn534fvb4uy4wrf7m2zcwpiycdzlebd6/bin/conda
no change     /opt/spack/opt/spack/linux-rocky9-x86_64/gcc-13.2.0/miniconda3-22.11.1-tn534fvb4uy4wrf7m2zcwpiycdzlebd6/bin/conda-env
no change     /opt/spack/opt/spack/linux-rocky9-x86_64/gcc-13.2.0/miniconda3-22.11.1-tn534fvb4uy4wrf7m2zcwpiycdzlebd6/bin/activate
no change     /opt/spack/opt/spack/linux-rocky9-x86_64/gcc-13.2.0/miniconda3-22.11.1-tn534fvb4uy4wrf7m2zcwpiycdzlebd6/bin/deactivate
no change     /opt/spack/opt/spack/linux-rocky9-x86_64/gcc-13.2.0/miniconda3-22.11.1-tn534fvb4uy4wrf7m2zcwpiycdzlebd6/etc/profile.d/conda.sh
no change     /opt/spack/opt/spack/linux-rocky9-x86_64/gcc-13.2.0/miniconda3-22.11.1-tn534fvb4uy4wrf7m2zcwpiycdzlebd6/etc/fish/conf.d/conda.fish
no change     /opt/spack/opt/spack/linux-rocky9-x86_64/gcc-13.2.0/miniconda3-22.11.1-tn534fvb4uy4wrf7m2zcwpiycdzlebd6/shell/condabin/Conda.psm1
no change     /opt/spack/opt/spack/linux-rocky9-x86_64/gcc-13.2.0/miniconda3-22.11.1-tn534fvb4uy4wrf7m2zcwpiycdzlebd6/shell/condabin/conda-hook.ps1
no change     /opt/spack/opt/spack/linux-rocky9-x86_64/gcc-13.2.0/miniconda3-22.11.1-tn534fvb4uy4wrf7m2zcwpiycdzlebd6/lib/python3.10/site-packages/xontrib/conda.xsh
no change     /opt/spack/opt/spack/linux-rocky9-x86_64/gcc-13.2.0/miniconda3-22.11.1-tn534fvb4uy4wrf7m2zcwpiycdzlebd6/etc/profile.d/conda.csh
no change     /u/scandussio/.bashrc
No action taken.
ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!
INFO 05-29 16:55:08 __init__.py:190] Automatically detected platform cuda.
==((====))==  Unsloth 2025.5.6: Fast Mistral patching. Transformers: 4.51.3. vLLM: 0.7.2.
   \\   /|    NVIDIA A100 80GB PCIe MIG 1g.20gb. Num GPUs = 1. Max memory: 19.5 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.5.1+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.1.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Dataset({
    features: ['system-prompt', 'prompt', 'answer', 'text'],
    num_rows: 180
})
Training starts
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 0.4151, 'grad_norm': 0.0246682520955801, 'learning_rate': 0.00019564245810055868, 'epoch': 0.28}
{'loss': 0.0023, 'grad_norm': 0.0009134543361142278, 'learning_rate': 0.00019005586592178773, 'epoch': 0.56}
{'loss': 0.0014, 'grad_norm': 0.00021257060870993882, 'learning_rate': 0.00018446927374301676, 'epoch': 0.83}
{'loss': 0.0014, 'grad_norm': 0.00016520229110028595, 'learning_rate': 0.00017888268156424582, 'epoch': 1.11}
{'loss': 0.0015, 'grad_norm': 0.0001370666577713564, 'learning_rate': 0.00017329608938547485, 'epoch': 1.39}
{'loss': 0.0015, 'grad_norm': 0.00012248307757545263, 'learning_rate': 0.0001677094972067039, 'epoch': 1.67}
{'loss': 0.0015, 'grad_norm': 9.615362796466798e-05, 'learning_rate': 0.00016212290502793297, 'epoch': 1.94}
{'loss': 0.0015, 'grad_norm': 8.683838677825406e-05, 'learning_rate': 0.000156536312849162, 'epoch': 2.22}
{'loss': 0.0015, 'grad_norm': 0.00010605144780129194, 'learning_rate': 0.00015094972067039106, 'epoch': 2.5}
{'loss': 0.0015, 'grad_norm': 8.545839955331758e-05, 'learning_rate': 0.00014536312849162012, 'epoch': 2.78}
{'loss': 0.0015, 'grad_norm': 8.796827751211822e-05, 'learning_rate': 0.00013977653631284918, 'epoch': 3.06}
{'loss': 0.0015, 'grad_norm': 9.576108277542517e-05, 'learning_rate': 0.0001341899441340782, 'epoch': 3.33}
{'loss': 0.0015, 'grad_norm': 0.00010224086145171896, 'learning_rate': 0.00012860335195530727, 'epoch': 3.61}
{'loss': 0.0015, 'grad_norm': 0.00012985331704840064, 'learning_rate': 0.00012301675977653633, 'epoch': 3.89}
{'loss': 0.0015, 'grad_norm': 9.284301631851122e-05, 'learning_rate': 0.00011743016759776537, 'epoch': 4.17}
{'loss': 0.0015, 'grad_norm': 0.00010319397551938891, 'learning_rate': 0.00011184357541899441, 'epoch': 4.44}
{'loss': 0.0016, 'grad_norm': 8.317457104567438e-05, 'learning_rate': 0.00010625698324022346, 'epoch': 4.72}
{'loss': 0.0016, 'grad_norm': 7.078916678437963e-05, 'learning_rate': 0.00010067039106145253, 'epoch': 5.0}
{'loss': 0.0016, 'grad_norm': 6.906176713528112e-05, 'learning_rate': 9.508379888268158e-05, 'epoch': 5.28}
{'loss': 0.0015, 'grad_norm': 7.775244739605114e-05, 'learning_rate': 8.949720670391062e-05, 'epoch': 5.56}
{'loss': 0.0015, 'grad_norm': 9.821748244576156e-05, 'learning_rate': 8.391061452513967e-05, 'epoch': 5.83}
{'loss': 0.0015, 'grad_norm': 8.399051148444414e-05, 'learning_rate': 7.832402234636872e-05, 'epoch': 6.11}
{'loss': 0.0016, 'grad_norm': 0.00011040973186027259, 'learning_rate': 7.273743016759777e-05, 'epoch': 6.39}
{'loss': 0.0016, 'grad_norm': 5.54498728888575e-05, 'learning_rate': 6.715083798882681e-05, 'epoch': 6.67}
{'loss': 0.0016, 'grad_norm': 8.469567546853796e-05, 'learning_rate': 6.156424581005586e-05, 'epoch': 6.94}
{'loss': 0.0016, 'grad_norm': 7.330383232329041e-05, 'learning_rate': 5.5977653631284924e-05, 'epoch': 7.22}
{'loss': 0.0016, 'grad_norm': 0.0001088072094717063, 'learning_rate': 5.039106145251397e-05, 'epoch': 7.5}
{'loss': 0.0016, 'grad_norm': 8.877905202098191e-05, 'learning_rate': 4.480446927374302e-05, 'epoch': 7.78}
{'loss': 0.0016, 'grad_norm': 0.00011719216854544356, 'learning_rate': 3.9217877094972065e-05, 'epoch': 8.06}
{'loss': 0.0016, 'grad_norm': 0.00010118601494468749, 'learning_rate': 3.3631284916201116e-05, 'epoch': 8.33}
{'loss': 0.0015, 'grad_norm': 9.481296729063615e-05, 'learning_rate': 2.8044692737430168e-05, 'epoch': 8.61}
{'loss': 0.0015, 'grad_norm': 0.00011662754695862532, 'learning_rate': 2.245810055865922e-05, 'epoch': 8.89}
{'loss': 0.0015, 'grad_norm': 0.00010521203512325883, 'learning_rate': 1.6871508379888268e-05, 'epoch': 9.17}
{'loss': 0.0016, 'grad_norm': 9.122316259890795e-05, 'learning_rate': 1.1284916201117319e-05, 'epoch': 9.44}
{'loss': 0.0015, 'grad_norm': 0.00014829731662757695, 'learning_rate': 5.698324022346369e-06, 'epoch': 9.72}
{'loss': 0.0015, 'grad_norm': 0.00010419782483950257, 'learning_rate': 1.1173184357541899e-07, 'epoch': 10.0}
{'train_runtime': 2821.4741, 'train_samples_per_second': 0.638, 'train_steps_per_second': 0.638, 'train_loss': 0.013037777890761694, 'epoch': 10.0}
Training ends
Unsloth: Merging 4bit and LoRA weights to 16bit...
Unsloth: Will use up to 521.59 out of 754.96 RAM for saving.
Unsloth: Saving model... This might take 5 minutes ...
Unsloth: Saving to organization with address saracandu/phi3-mini-rebus-solver-coldstart
Unsloth: Saving tokenizer... Done.
Unsloth: Saving to organization with address saracandu/phi3-mini-rebus-solver-coldstart
Unsloth: Uploading all files... Please wait...
Done.
Saved merged model to https://huggingface.co/None/phi3-mini-rebus-solver-coldstart
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mzesty-river-5[0m at: [34mhttps://wandb.ai/saracandussio-universit-degli-studi-di-trieste/cold-start-phi/runs/whahrvze[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250529_165524-whahrvze/logs[0m
DONE!
