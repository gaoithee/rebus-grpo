==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 81,318 | Num Epochs = 3 | Total steps = 30,492
O^O/ \_/ \    Batch size per device = 6 | Gradient accumulation steps = 8
\        /    Data Parallel GPUs = 1 | Total batch size (6 x 8 x 1) = 48
 "-____-"     Trainable parameters = 41,943,040/8,000,000,000 (0.52% trained)
[34m[1mwandb[0m: [33mWARNING[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
  0%|                                                                                     | 0/30492 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/u/scandussio/rebus-grpo/grpo-llama.py", line 73, in <module>
    trainer.train()
  File "/u/scandussio/.conda/envs/rebus-env/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
    return inner_training_loop(
  File "<string>", line 314, in _fast_inner_training_loop
  File "<string>", line 25, in _unsloth_training_step
  File "/u/scandussio/rebus-grpo/unsloth_compiled_cache/UnslothGRPOTrainer.py", line 972, in _prepare_inputs
    prompt_completion_ids = unwrapped_model.generate(
  File "/u/scandussio/.conda/envs/rebus-env/lib/python3.10/site-packages/unsloth/models/rl.py", line 69, in generate_with_clone
    out = original_generate(*args, **kwargs)
  File "/u/scandussio/.conda/envs/rebus-env/lib/python3.10/site-packages/peft/peft_model.py", line 1875, in generate
    outputs = self.base_model.generate(*args, **kwargs)
  File "/u/scandussio/.conda/envs/rebus-env/lib/python3.10/site-packages/unsloth/models/llama.py", line 1581, in unsloth_fast_generate
    output = self._old_generate(*args, **kwargs)
  File "/u/scandussio/.conda/envs/rebus-env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/u/scandussio/.conda/envs/rebus-env/lib/python3.10/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
  File "/u/scandussio/.conda/envs/rebus-env/lib/python3.10/site-packages/transformers/generation/utils.py", line 3434, in _sample
    outputs = model_forward(**model_inputs, return_dict=True)
  File "/u/scandussio/.conda/envs/rebus-env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/u/scandussio/.conda/envs/rebus-env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/u/scandussio/.conda/envs/rebus-env/lib/python3.10/site-packages/unsloth/models/llama.py", line 1033, in _CausalLM_fast_forward
    outputs = fast_forward_inference(
  File "/u/scandussio/.conda/envs/rebus-env/lib/python3.10/site-packages/unsloth/models/llama.py", line 966, in LlamaModel_fast_forward_inference_custom
    X, present_key_value = attention_fast_forward_inference(
  File "/u/scandussio/.conda/envs/rebus-env/lib/python3.10/site-packages/unsloth/models/llama.py", line 222, in LlamaAttention_fast_forward_inference
    cos = cos[position_ids].unsqueeze(1)
KeyboardInterrupt
