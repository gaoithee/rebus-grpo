Training begins...
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 81,318 | Num Epochs = 1 | Total steps = 6,776
O^O/ \_/ \    Batch size per device = 12 | Gradient accumulation steps = 6
\        /    Data Parallel GPUs = 1 | Total batch size (12 x 6 x 1) = 72
 "-____-"     Trainable parameters = 29,884,416/4,000,000,000 (0.75% trained)
wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
  0%|          | 14/6776 [34:36<277:46:35, 147.88s/it]
Unsloth: Will smartly offload gradients to save VRAM!
