{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16177c77-1367-4623-83a9-e42780e06ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Gemma2 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 7.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.3.19 patched 26 layers with 26 QKV layers, 26 O layers and 26 MLP layers.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Gemma2ForCausalLM(\n",
       "      (model): Gemma2Model(\n",
       "        (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-25): 26 x Gemma2DecoderLayer(\n",
       "            (self_attn): Gemma2Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2304, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2304, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2304, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2304, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2304, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): GemmaFixedRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Gemma2MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2304, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=9216, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2304, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=9216, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=9216, out_features=2304, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=9216, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): PytorchGELUTanh()\n",
       "            )\n",
       "            (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "            (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "            (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "            (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (rotary_emb): GemmaFixedRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 1248 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model_type = \"gemma\" # llama, phi-3, gemma\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"gsarti/gemma-2-2b-rebus-solver-adapters\", # MODEL OR ADAPTER FOLDER\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e08c24cd-f69f-4a07-827a-a924cd7e4dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "eval_dataset = load_dataset('gsarti/eureka-rebus', 'llm_sft', data_files=[\"id_test.jsonl\", \"ood_test.jsonl\"], split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2c270e9-4145-40f2-ba84-45f906585938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single-example generation\n",
    "\n",
    "stop_token_id = model.config.eos_token_id\n",
    "if model_type == \"gemma\":\n",
    "    stop_token = \"<|eot_id|>\"\n",
    "    stop_token_id = tokenizer.encode(stop_token)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b63b6c09-15fd-4769-be32-41f05117d38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "if model_type == \"llama\":\n",
    "    tokenizer.padding_side = \"right\"\n",
    "elif model_type in (\"phi-3\", \"gemma\"):\n",
    "    tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44647cf2-fb92-422d-8dac-df5abb094ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "regex_word_guess = '- \\[.* = (.*)'\n",
    "regex_firstpass = 'Prima lettura: (.*)'\n",
    "regex_solution_word = \"\\d+ = (.*)\"\n",
    "regex_solution = \"Soluzione: (.*)\"\n",
    "\n",
    "def parse_generation(data):\n",
    "    try:\n",
    "        word_guesses = \";\".join(re.findall(regex_word_guess, data))\n",
    "    except:\n",
    "        word_guesses = \"\"\n",
    "    try:\n",
    "        first_pass = re.findall(regex_firstpass, data)[0]\n",
    "    except:\n",
    "        first_pass = \"\"\n",
    "    try:\n",
    "        solution_words = \";\".join(re.findall(regex_solution_word, data))\n",
    "    except:\n",
    "        solution_words = \"\"\n",
    "    try:\n",
    "        solution = re.findall(regex_solution, data)[0]\n",
    "    except:\n",
    "        solution = \"\"\n",
    "    \n",
    "    return {\n",
    "        \"word_guesses\": word_guesses,\n",
    "        \"first_pass\": first_pass,\n",
    "        \"solution_words\": solution_words,\n",
    "        \"solution\": solution,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6cb4fe95-aab3-4d38-9b65-cb28e8c9851d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_dataset['conversations'][5][0]['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "173522d1-def2-4359-98f0-66a9a67241bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse_generation(eval_dataset['conversations'][5][1]['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e94f36f2-ea7c-4044-acfc-f1fe407db6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = tokenizer(eval_dataset['conversations'][5][0]['value'], return_tensors=\"pt\")[\"input_ids\"].to('cuda:0')\n",
    "# outputs = model.generate(input_ids = inputs, max_new_tokens = 500, use_cache = True, eos_token_id = stop_token_id)\n",
    "# model_generations = tokenizer.batch_decode(outputs)\n",
    "# print(model_generations[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb6cd5cd-2cc3-4cc1-b921-9a93410f0d83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from unsloth import FastLanguageModel\n",
    "\n",
    "# max_seq_length = 1248 # Choose any! We auto support RoPE Scaling internally!\n",
    "# dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "# load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# model_type = \"gemma\" # llama, phi-3, gemma\n",
    "\n",
    "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#     model_name = \"gsarti/gemma-2-2b-rebus-solver-fp16\", # MODEL OR ADAPTER FOLDER\n",
    "#     max_seq_length = max_seq_length,\n",
    "#     dtype = dtype,\n",
    "#     load_in_4bit = load_in_4bit,\n",
    "# )\n",
    "# FastLanguageModel.for_inference(model)\n",
    "\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# eval_dataset = load_dataset('gsarti/eureka-rebus', 'llm_sft', data_files=[\"id_test.jsonl\", \"ood_test.jsonl\"], split = \"train\")\n",
    "\n",
    "# # Single-example generation\n",
    "\n",
    "# stop_token_id = model.config.eos_token_id\n",
    "# if model_type == \"gemma\":\n",
    "#     stop_token = \"<|eot_id|>\"\n",
    "#     stop_token_id = tokenizer.encode(stop_token)[0]\n",
    "\n",
    "# ex_idx = 5\n",
    "# example = eval_dataset[ex_idx][\"conversations\"][0]\n",
    "# inputs = tokenizer.apply_chat_template(\n",
    "#     [\n",
    "#         {\"role\": \"user\", \"content\": example[\"value\"]}\n",
    "#     ],\n",
    "#     add_generation_prompt=True,\n",
    "#     return_tensors = \"pt\",\n",
    "#     padding=True,\n",
    "#     truncation=True,\n",
    "# )\n",
    "# inputs = inputs.to('cuda:0')\n",
    "# outputs = model.generate(input_ids = inputs, max_new_tokens = 500, use_cache = True, eos_token_id = stop_token_id)\n",
    "# model_generations = tokenizer.batch_decode(outputs)\n",
    "# print(model_generations[0])\n",
    "\n",
    "\n",
    "\n",
    "# import re\n",
    "\n",
    "# regex_word_guess = '- \\[.* = (.*)'\n",
    "# regex_firstpass = 'Prima lettura: (.*)'\n",
    "# regex_solution_word = \"\\d+ = (.*)\"\n",
    "# regex_solution = \"Soluzione: (.*)\"\n",
    "\n",
    "# def parse_generation(ex_idx, ex):\n",
    "#     try:\n",
    "#         word_guesses = \";\".join(re.findall(regex_word_guess, ex))\n",
    "#     except:\n",
    "#         word_guesses = \"\"\n",
    "#     try:\n",
    "#         first_pass = re.findall(regex_firstpass, ex)[0]\n",
    "#     except:\n",
    "#         first_pass = \"\"\n",
    "#     try:\n",
    "#         solution_words = \";\".join(re.findall(regex_solution_word, ex))\n",
    "#     except:\n",
    "#         solution_words = \"\"\n",
    "#     try:\n",
    "#         solution = re.findall(regex_solution, ex)[0]\n",
    "#     except:\n",
    "#         solution = \"\"\n",
    "#     return {\n",
    "#         \"idx\": ex_idx,\n",
    "#         \"word_guesses\": word_guesses,\n",
    "#         \"first_pass\": first_pass,\n",
    "#         \"solution_words\": solution_words,\n",
    "#         \"solution\": solution,\n",
    "#     }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Create gold parsed outputs\n",
    "\n",
    "# results = []\n",
    "# for ex_idx, ex in tqdm(enumerate(eval_dataset), total=len(eval_dataset)):\n",
    "#     gold_output = ex[\"conversations\"][1][\"value\"]\n",
    "#     parsed_output = parse_generation(ex_idx, gold_output)\n",
    "#     results.append(parsed_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44de2a7c-fffc-4422-aa6e-b2c5a2a68474",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# if model_type == \"llama\":\n",
    "#     tokenizer.padding_side = \"right\"\n",
    "# elif model_type in (\"phi-3\", \"gemma\"):\n",
    "#     tokenizer.padding_side = \"left\"\n",
    "\n",
    "# results = []\n",
    "# batch_size = 134\n",
    "# for i in tqdm(range(0, len(eval_dataset), batch_size), total=len(eval_dataset)//batch_size):\n",
    "#     batch = eval_dataset[i:i+batch_size]\n",
    "#     print(batch['conversations'][0])\n",
    "#     if model_type == \"llama\":\n",
    "#         input = [[{\"role\": \"user\", \"content\": example[0][\"value\"]}] for example in batch[\"conversations\"]]\n",
    "    # elif model_type == \"phi-3\":\n",
    "        # print(batch['conversation'])\n",
    "        # input = [[item[0]] for item in batch[\"conversations\"]]\n",
    "\n",
    "    # inputs = tokenizer.apply_chat_template(\n",
    "    #     input,\n",
    "    #     tokenize = True,\n",
    "    #     add_generation_prompt = True, # Must add for generation\n",
    "    #     return_tensors = \"pt\",\n",
    "    #     padding=True,\n",
    "    #     truncation=True,\n",
    "    #     return_dict=True\n",
    "    # )\n",
    "    # inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "    # outputs = model.generate(**inputs, max_new_tokens = 500, use_cache = True, eos_token_id = stop_token_id)\n",
    "\n",
    "    # model_generations = tokenizer.batch_decode(outputs)\n",
    "    # for ex_idx, ex in enumerate(model_generations):\n",
    "    #     out_dic = parse_generation(ex_idx + i, ex)\n",
    "    #     if i == 0 and ex_idx <= 5:\n",
    "    #         print(ex)\n",
    "    #         print(out_dic)\n",
    "    #     results.append(out_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfe87f7-c6b5-40fe-b713-5182c496a2ec",
   "metadata": {},
   "source": [
    "# qui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12710559-188b-4176-b03b-293281359f67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word_guesses': 'pire;cent',\n",
       " 'first_pass': 'TEM pire cent I',\n",
       " 'solution_words': 'Tempi;recenti',\n",
       " 'solution': 'Tempi recenti'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Single-example generation\n",
    "\n",
    "stop_token_id = model.config.eos_token_id\n",
    "if model_type == \"gemma\":\n",
    "    stop_token = \"<|eot_id|>\"\n",
    "    stop_token_id = tokenizer.encode(stop_token)[0]\n",
    "\n",
    "ex_idx = 5\n",
    "example = eval_dataset[ex_idx][\"conversations\"][0]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": example[\"value\"]}\n",
    "    ],\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors = \"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    ")\n",
    "inputs = inputs.to('cuda:0')\n",
    "outputs = model.generate(input_ids = inputs, max_new_tokens = 500, use_cache = True, eos_token_id = stop_token_id)\n",
    "model_generations = tokenizer.batch_decode(outputs)\n",
    "parse_generation(model_generations[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c90a90b-9ee2-4c92-b619-85ce7fcb1988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1998"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fa83bbd7-4c7a-4e1c-926b-58053ec98e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversations'],\n",
       "    num_rows: 1998\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b414b50d-24b0-4728-93fe-70e4c6514b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['conversations'],\n",
      "    num_rows: 10\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Seleziona un subset di dimensione 10\n",
    "subset = eval_dataset.select(range(10))\n",
    "\n",
    "# Verifica il subset\n",
    "print(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d464efb6-5169-4734-9457-5dda3ef338a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████                                                                        | 1/10 [00:21<03:11, 21.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "Risolvi gli indizi tra parentesi per ottenere una prima lettura, e usa la chiave di lettura per ottenere la soluzione del rebus.\n",
      "\n",
      "Rebus: [Sono pari nell' odio] C CI [Altro nome dei gicheri] NF [Se scappa, va in esilio] SC [Le ha l'armadio]\n",
      "Chiave di lettura: 6 12<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Procediamo alla risoluzione del rebus passo per passo:\n",
      "- [Sono pari nell' odio] = do\n",
      "- C C I = C C I\n",
      "- [Altro nome dei gicheri] = ari\n",
      "- N F = N F\n",
      "- [Se scappa, va in esilio] = re\n",
      "- S C = S C\n",
      "- [Le ha l'armadio] = ante\n",
      "\n",
      "Prima lettura: do C CI ari NF re SC ante\n",
      "\n",
      "Ora componiamo la soluzione seguendo la chiave risolutiva:\n",
      "6 = Doccia\n",
      "12 = rinfrescante\n",
      "\n",
      "Soluzione: Doccia rinfrescante\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████                                                                | 2/10 [00:42<02:49, 21.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "Risolvi gli indizi tra parentesi per ottenere una prima lettura, e usa la chiave di lettura per ottenere la soluzione del rebus.\n",
      "\n",
      "Rebus: A [Un essere strisciante] M [Costosi ornamenti] [È a destra o a sinistra] C UN [Audace, ardimentosa]\n",
      "Chiave di lettura: 4 7 8<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Procediamo alla risoluzione del rebus passo per passo:\n",
      "- A = A\n",
      "- [Un essere strisciante] = verme\n",
      "- M = M\n",
      "- [Costosi ornamenti] = ori\n",
      "- [È a destra o a sinistra] = ala\n",
      "- C U N = C U N\n",
      "- [Audace, ardimentosa] = osa\n",
      "\n",
      "Prima lettura: A verme M ori ala C UN osa\n",
      "\n",
      "Ora componiamo la soluzione seguendo la chiave risolutiva:\n",
      "4 = Aver\n",
      "7 = memoria\n",
      "8 = lacunosa\n",
      "\n",
      "Soluzione: Aver memoria lacunosa\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████                                                        | 3/10 [01:03<02:28, 21.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "Risolvi gli indizi tra parentesi per ottenere una prima lettura, e usa la chiave di lettura per ottenere la soluzione del rebus.\n",
      "\n",
      "Rebus: A [Quello di fiori si riceve volentieri] N [Si desta se si grida] [La metà del diametro] SA\n",
      "Chiave di lettura: 8 10<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Procediamo alla risoluzione del rebus passo per passo:\n",
      "- A = A\n",
      "- [Quello di fiori si riceve volentieri] = mazzo\n",
      "- N = N\n",
      "- [Si desta se si grida] = eco\n",
      "- [La metà del diametro] = raggio\n",
      "- S A = S A\n",
      "\n",
      "Prima lettura: A mazzo N eco raggio SA\n",
      "\n",
      "Ora componiamo la soluzione seguendo la chiave risolutiva:\n",
      "8 = Amazzone\n",
      "10 = coraggiosa\n",
      "\n",
      "Soluzione: Amazzone coraggiosa\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████████████████████                                                | 4/10 [01:25<02:07, 21.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "Risolvi gli indizi tra parentesi per ottenere una prima lettura, e usa la chiave di lettura per ottenere la soluzione del rebus.\n",
      "\n",
      "Rebus: P [Può abdicare] TE [Uno scrittore come Cesare Cantù] NO [Usati sulla neve] U [Si ottiene sommando]\n",
      "Chiave di lettura: 8 12 4<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Procediamo alla risoluzione del rebus passo per passo:\n",
      "- P = P\n",
      "- [Può abdicare] = re\n",
      "- T E = T E\n",
      "- [Uno scrittore come Cesare Cantù] = storico\n",
      "- N O = N O\n",
      "- [Usati sulla neve] = sci\n",
      "- U = U\n",
      "- [Si ottiene sommando] = totale\n",
      "\n",
      "Prima lettura: P re TE storico NO sci U totale\n",
      "\n",
      "Ora componiamo la soluzione seguendo la chiave risolutiva:\n",
      "8 = Pretesto\n",
      "12 = riconosciuto\n",
      "4 = tale\n",
      "\n",
      "Soluzione: Pretesto riconosciuto tale\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████████████████████████████████████████                                        | 5/10 [01:46<01:46, 21.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "Risolvi gli indizi tra parentesi per ottenere una prima lettura, e usa la chiave di lettura per ottenere la soluzione del rebus.\n",
      "\n",
      "Rebus: N [Le tracce per l' inseguitore] [Abili] M [I gioielli di famiglia] G [Sono piccole di notte]\n",
      "Chiave di lettura: 5 1 ' 7 6<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Procediamo alla risoluzione del rebus passo per passo:\n",
      "- N = N\n",
      "- [Le tracce per l' inseguitore] = orme\n",
      "- [Abili] = destre\n",
      "- M = M\n",
      "- [I gioielli di famiglia] = ori\n",
      "- G = G\n",
      "- [Sono piccole di notte] = ore\n",
      "\n",
      "Prima lettura: N orme destre M ori G ore\n",
      "\n",
      "Ora componiamo la soluzione seguendo la chiave risolutiva:\n",
      "5 = Norme\n",
      "1 = d\n",
      "' = '\n",
      "7 = estremo\n",
      "6 = rigore\n",
      "\n",
      "Soluzione: Norme d'estremo rigore\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████████████████████████████████████                                | 6/10 [02:07<01:25, 21.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "Risolvi gli indizi tra parentesi per ottenere una prima lettura, e usa la chiave di lettura per ottenere la soluzione del rebus.\n",
      "\n",
      "Rebus: TEM [Cataste da ardere] [Una parte del dollaro] I\n",
      "Chiave di lettura: 5 7<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Procediamo alla risoluzione del rebus passo per passo:\n",
      "- T E M = T E M\n",
      "- [Cataste da ardere] = pire\n",
      "- [Una parte del dollaro] = cent\n",
      "- I = I\n",
      "\n",
      "Prima lettura: TEM pire cent I\n",
      "\n",
      "Ora componiamo la soluzione seguendo la chiave risolutiva:\n",
      "5 = Tempi\n",
      "7 = recenti\n",
      "\n",
      "Soluzione: Tempi recenti\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|████████████████████████████████████████████████████████                        | 7/10 [02:28<01:03, 21.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "Risolvi gli indizi tra parentesi per ottenere una prima lettura, e usa la chiave di lettura per ottenere la soluzione del rebus.\n",
      "\n",
      "Rebus: CI [Il Cristoforo de I promessi sposi] [Capocuoco alla francese] A [Il secondo numero primo] [Vi ci si muove a nuoto]\n",
      "Chiave di lettura: 5 3 2 7<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Procediamo alla risoluzione del rebus passo per passo:\n",
      "- C I = C I\n",
      "- [Il Cristoforo de I promessi sposi] = fra\n",
      "- [Capocuoco alla francese] = chef\n",
      "- A = A\n",
      "- [Il secondo numero primo] = tre\n",
      "- [Vi ci si muove a nuoto] = mare\n",
      "\n",
      "Prima lettura: CI fra chef A tre mare\n",
      "\n",
      "Ora componiamo la soluzione seguendo la chiave risolutiva:\n",
      "5 = Cifra\n",
      "3 = che\n",
      "2 = fa\n",
      "7 = tremare\n",
      "\n",
      "Soluzione: Cifra che fa tremare\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████████████████████████████████████████                | 8/10 [02:50<00:42, 21.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "Risolvi gli indizi tra parentesi per ottenere una prima lettura, e usa la chiave di lettura per ottenere la soluzione del rebus.\n",
      "\n",
      "Rebus: TR [La compagna di Adamo] L [Sono fatti per il riposo] M [Sono ambiti alle Olimpiadi]\n",
      "Chiave di lettura: 3 8 4<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Procediamo alla risoluzione del rebus passo per passo:\n",
      "- T R = T R\n",
      "- [La compagna di Adamo] = Eva\n",
      "- L = L\n",
      "- [Sono fatti per il riposo] = letti\n",
      "- M = M\n",
      "- [Sono ambiti alle Olimpiadi] = ori\n",
      "\n",
      "Prima lettura: TR Eva L letti M ori\n",
      "\n",
      "Ora componiamo la soluzione seguendo la chiave risolutiva:\n",
      "3 = Tre\n",
      "8 = valletti\n",
      "4 = mori\n",
      "\n",
      "Soluzione: Tre valletti mori\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████████████████████████████████████████████████████████████████████        | 9/10 [03:11<00:21, 21.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "Risolvi gli indizi tra parentesi per ottenere una prima lettura, e usa la chiave di lettura per ottenere la soluzione del rebus.\n",
      "\n",
      "Rebus: R [Altare o pappagallo] I [Preposizione di accompagnamento] A\n",
      "Chiave di lettura: 4 5<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Procediamo alla risoluzione del rebus passo per passo:\n",
      "- R = R\n",
      "- [Altare o pappagallo] = ara\n",
      "- I = I\n",
      "- [Preposizione di accompagnamento] = con\n",
      "- A = A\n",
      "\n",
      "Prima lettura: R ara I con A\n",
      "\n",
      "Ora componiamo la soluzione seguendo la chiave risolutiva:\n",
      "4 = Rara\n",
      "5 = icona\n",
      "\n",
      "Soluzione: Rara icona\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 10/10 [03:32<00:00, 21.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "Risolvi gli indizi tra parentesi per ottenere una prima lettura, e usa la chiave di lettura per ottenere la soluzione del rebus.\n",
      "\n",
      "Rebus: [Preposizione intermedia] [Popolare Ombretta dello spettacolo] F A [Questi]\n",
      "Chiave di lettura: 8 6<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Procediamo alla risoluzione del rebus passo per passo:\n",
      "- [Preposizione intermedia] = tra\n",
      "- [Popolare Ombretta dello spettacolo] = colli\n",
      "- F A = F A\n",
      "- [Questi] = tali\n",
      "\n",
      "Prima lettura: tra colli F A tali\n",
      "\n",
      "Ora componiamo la soluzione seguendo la chiave risolutiva:\n",
      "8 = Tracolli\n",
      "6 = fatali\n",
      "\n",
      "Soluzione: Tracolli fatali\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n",
      "<\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for ex_idx in tqdm(range(len(subset)), total=len(subset)):\n",
    "    # Ottieni il primo esempio di conversazione (senza batch)\n",
    "    examples = subset[ex_idx]\n",
    "    prompts = examples['conversations'][0]\n",
    "\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": prompts[\"value\"]}\n",
    "    ],\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors = \"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    )\n",
    "    inputs = inputs.to('cuda:0')\n",
    "    \n",
    "    # print(example)  # Puoi usare questo per controllare il contenuto dell'esempio\n",
    "\n",
    "    # input = [[item[0]] for item in examples]\n",
    "    \n",
    "    # # A questo punto puoi fare inferenza su `input`, ad esempio:\n",
    "    # inputs = tokenizer.apply_chat_template(\n",
    "    #     input,\n",
    "    #     tokenize=True,\n",
    "    #     add_generation_prompt=True,  # Aggiungi il prompt per la generazione\n",
    "    #     return_tensors=\"pt\",\n",
    "    #     padding=True,\n",
    "    #     truncation=True,\n",
    "    #     return_dict=True\n",
    "    # )\n",
    "\n",
    "    # # Inferenza sul modello\n",
    "    outputs = model.generate(inputs, max_new_tokens=500, use_cache=True, eos_token_id=stop_token_id)\n",
    "    model_generations = tokenizer.batch_decode(outputs)\n",
    "    \n",
    "    # # Aggiungi il risultato alla lista dei risultati\n",
    "    results.append(parse_generation(model_generations[0]))\n",
    "    \n",
    "    # # Opzionalmente, puoi fare qualcosa con i risultati, come stamparli:\n",
    "    print(model_generations[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fcb0968a-c44a-46e7-9ec6-58af74e79c0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word_guesses': 'do;ari;re;ante',\n",
       "  'first_pass': 'do C CI ari NF re SC ante',\n",
       "  'solution_words': 'Doccia;rinfrescante',\n",
       "  'solution': 'Doccia rinfrescante'},\n",
       " {'word_guesses': 'verme;ori;ala;osa',\n",
       "  'first_pass': 'A verme M ori ala C UN osa',\n",
       "  'solution_words': 'Aver;memoria;lacunosa',\n",
       "  'solution': 'Aver memoria lacunosa'},\n",
       " {'word_guesses': 'mazzo;eco;raggio',\n",
       "  'first_pass': 'A mazzo N eco raggio SA',\n",
       "  'solution_words': 'Amazzone;coraggiosa',\n",
       "  'solution': 'Amazzone coraggiosa'},\n",
       " {'word_guesses': 're;storico;sci;totale',\n",
       "  'first_pass': 'P re TE storico NO sci U totale',\n",
       "  'solution_words': 'Pretesto;riconosciuto;tale',\n",
       "  'solution': 'Pretesto riconosciuto tale'},\n",
       " {'word_guesses': 'orme;destre;ori;ore',\n",
       "  'first_pass': 'N orme destre M ori G ore',\n",
       "  'solution_words': 'Norme;d;estremo;rigore',\n",
       "  'solution': \"Norme d'estremo rigore\"},\n",
       " {'word_guesses': 'pire;cent',\n",
       "  'first_pass': 'TEM pire cent I',\n",
       "  'solution_words': 'Tempi;recenti',\n",
       "  'solution': 'Tempi recenti'},\n",
       " {'word_guesses': 'fra;chef;tre;mare',\n",
       "  'first_pass': 'CI fra chef A tre mare',\n",
       "  'solution_words': 'Cifra;che;fa;tremare',\n",
       "  'solution': 'Cifra che fa tremare'},\n",
       " {'word_guesses': 'Eva;letti;ori',\n",
       "  'first_pass': 'TR Eva L letti M ori',\n",
       "  'solution_words': 'Tre;valletti;mori',\n",
       "  'solution': 'Tre valletti mori'},\n",
       " {'word_guesses': 'ara;con',\n",
       "  'first_pass': 'R ara I con A',\n",
       "  'solution_words': 'Rara;icona',\n",
       "  'solution': 'Rara icona'},\n",
       " {'word_guesses': 'tra;colli;tali',\n",
       "  'first_pass': 'tra colli F A tali',\n",
       "  'solution_words': 'Tracolli;fatali',\n",
       "  'solution': 'Tracolli fatali'}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742a8d4b-d2ee-4967-88f2-d76f0498dd2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c28e27-a57d-418f-bb4c-3861bbc29d36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
