{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b20c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 1248 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model_type = \"gemma\" # llama, phi-3, gemma\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"gsarti/gemma-2-2b-rebus-solver-adapters\", # MODEL OR ADAPTER FOLDER\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4e8219",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_token_id = model.config.eos_token_id\n",
    "if model_type == \"gemma\":\n",
    "    stop_token = \"<|eot_id|>\"\n",
    "    stop_token_id = tokenizer.encode(stop_token)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc09171",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "if model_type == \"llama\":\n",
    "    tokenizer.padding_side = \"right\"\n",
    "elif model_type in (\"phi-3\", \"gemma\"):\n",
    "    tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86048912",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('gsarti/eureka-rebus', 'llm_sft', data_files=[\"train.jsonl\"], split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca548bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTA: qua dovrai sicuro cambiare un po' di roba\n",
    "\n",
    "ex_idx = 5\n",
    "example = dataset[ex_idx][\"conversations\"][0]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": example[\"value\"]}\n",
    "    ],\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors = \"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    ")\n",
    "inputs = inputs.to('cuda:0')\n",
    "\n",
    "# l'idea secondo me Ã¨ che `inputs` va passato a GRPOTrainer, ma controlla!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4564531a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qua a occhio resta tutto tale e quale\n",
    "\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "max_prompt_length = 256\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    learning_rate=5e-6,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.99,\n",
    "    weight_decay=0.1,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    logging_steps=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,  # Increase to 4 for smoother training\n",
    "    num_generations=6,  # Decrease if out of memory\n",
    "    max_prompt_length=max_prompt_length,\n",
    "    max_completion_length=500,\n",
    "    # num_train_epochs = 1, # Set to 1 for a full training run\n",
    "    max_steps=250,\n",
    "    save_steps=250,\n",
    "    max_grad_norm=0.1,\n",
    "    report_to=\"none\",  # Can use Weights & Biases\n",
    "    output_dir=\"outputs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918494e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    reward_funcs=...,\n",
    "    args=training_args,\n",
    "    train_dataset=...,\n",
    ")\n",
    "\n",
    "# QUA SECONDO ME DEVI RI-DEFINIRE UN TRAINING DATASET CON LA FORMATTAZIONE DI SOPRA\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
