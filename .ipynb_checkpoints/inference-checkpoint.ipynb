{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16177c77-1367-4623-83a9-e42780e06ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 21:33:30.705681: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-12 21:33:30.747748: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-12 21:33:30.747785: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-12 21:33:30.749037: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-12 21:33:30.756921: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-12 21:33:31.831168: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Failed to patch SmolVLMForConditionalGeneration forward function.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.3.19: Fast Mistral patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 7.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.3.19 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): MistralForCausalLM(\n",
       "      (model): MistralModel(\n",
       "        (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x MistralDecoderLayer(\n",
       "            (self_attn): MistralAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): MistralMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): MistralRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): MistralRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): MistralRMSNorm((3072,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 1248 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model_type = \"phi-3\" # llama, phi-3, gemma\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"gsarti/phi3-mini-rebus-solver-adapters\", # MODEL OR ADAPTER FOLDER\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e08c24cd-f69f-4a07-827a-a924cd7e4dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "eval_dataset = load_dataset('gsarti/eureka-rebus', 'llm_sft', data_files=[\"id_test.jsonl\", \"ood_test.jsonl\"], split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2c270e9-4145-40f2-ba84-45f906585938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single-example generation\n",
    "\n",
    "stop_token_id = model.config.eos_token_id\n",
    "if model_type == \"gemma\":\n",
    "    stop_token = \"<|eot_id|>\"\n",
    "    stop_token_id = tokenizer.encode(stop_token)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b63b6c09-15fd-4769-be32-41f05117d38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "if model_type == \"llama\":\n",
    "    tokenizer.padding_side = \"right\"\n",
    "elif model_type in (\"phi-3\", \"gemma\"):\n",
    "    tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44647cf2-fb92-422d-8dac-df5abb094ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "regex_word_guess = '- \\[.* = (.*)'\n",
    "regex_firstpass = 'Prima lettura: (.*)'\n",
    "regex_solution_word = \"\\d+ = (.*)\"\n",
    "regex_solution = \"Soluzione: (.*)\"\n",
    "\n",
    "def parse_generation(data):\n",
    "    try:\n",
    "        word_guesses = \";\".join(re.findall(regex_word_guess, data))\n",
    "    except:\n",
    "        word_guesses = \"\"\n",
    "    try:\n",
    "        first_pass = re.findall(regex_firstpass, data)[0]\n",
    "    except:\n",
    "        first_pass = \"\"\n",
    "    try:\n",
    "        solution_words = \";\".join(re.findall(regex_solution_word, data))\n",
    "    except:\n",
    "        solution_words = \"\"\n",
    "    try:\n",
    "        solution = re.findall(regex_solution, data)[0]\n",
    "    except:\n",
    "        solution = \"\"\n",
    "    \n",
    "    return {\n",
    "        \"word_guesses\": word_guesses,\n",
    "        \"first_pass\": first_pass,\n",
    "        \"solution_words\": solution_words,\n",
    "        \"solution\": solution,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cb4fe95-aab3-4d38-9b65-cb28e8c9851d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_dataset['conversations'][5][0]['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "173522d1-def2-4359-98f0-66a9a67241bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse_generation(eval_dataset['conversations'][5][1]['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e94f36f2-ea7c-4044-acfc-f1fe407db6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = tokenizer(eval_dataset['conversations'][5][0]['value'], return_tensors=\"pt\")[\"input_ids\"].to('cuda:0')\n",
    "# outputs = model.generate(input_ids = inputs, max_new_tokens = 500, use_cache = True, eos_token_id = stop_token_id)\n",
    "# model_generations = tokenizer.batch_decode(outputs)\n",
    "# print(model_generations[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6cd5cd-2cc3-4cc1-b921-9a93410f0d83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from unsloth import FastLanguageModel\n",
    "\n",
    "# max_seq_length = 1248 # Choose any! We auto support RoPE Scaling internally!\n",
    "# dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "# load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# model_type = \"gemma\" # llama, phi-3, gemma\n",
    "\n",
    "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#     model_name = \"gsarti/gemma-2-2b-rebus-solver-fp16\", # MODEL OR ADAPTER FOLDER\n",
    "#     max_seq_length = max_seq_length,\n",
    "#     dtype = dtype,\n",
    "#     load_in_4bit = load_in_4bit,\n",
    "# )\n",
    "# FastLanguageModel.for_inference(model)\n",
    "\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# eval_dataset = load_dataset('gsarti/eureka-rebus', 'llm_sft', data_files=[\"id_test.jsonl\", \"ood_test.jsonl\"], split = \"train\")\n",
    "\n",
    "# # Single-example generation\n",
    "\n",
    "# stop_token_id = model.config.eos_token_id\n",
    "# if model_type == \"gemma\":\n",
    "#     stop_token = \"<|eot_id|>\"\n",
    "#     stop_token_id = tokenizer.encode(stop_token)[0]\n",
    "\n",
    "# ex_idx = 5\n",
    "# example = eval_dataset[ex_idx][\"conversations\"][0]\n",
    "# inputs = tokenizer.apply_chat_template(\n",
    "#     [\n",
    "#         {\"role\": \"user\", \"content\": example[\"value\"]}\n",
    "#     ],\n",
    "#     add_generation_prompt=True,\n",
    "#     return_tensors = \"pt\",\n",
    "#     padding=True,\n",
    "#     truncation=True,\n",
    "# )\n",
    "# inputs = inputs.to('cuda:0')\n",
    "# outputs = model.generate(input_ids = inputs, max_new_tokens = 500, use_cache = True, eos_token_id = stop_token_id)\n",
    "# model_generations = tokenizer.batch_decode(outputs)\n",
    "# print(model_generations[0])\n",
    "\n",
    "\n",
    "\n",
    "# import re\n",
    "\n",
    "# regex_word_guess = '- \\[.* = (.*)'\n",
    "# regex_firstpass = 'Prima lettura: (.*)'\n",
    "# regex_solution_word = \"\\d+ = (.*)\"\n",
    "# regex_solution = \"Soluzione: (.*)\"\n",
    "\n",
    "# def parse_generation(ex_idx, ex):\n",
    "#     try:\n",
    "#         word_guesses = \";\".join(re.findall(regex_word_guess, ex))\n",
    "#     except:\n",
    "#         word_guesses = \"\"\n",
    "#     try:\n",
    "#         first_pass = re.findall(regex_firstpass, ex)[0]\n",
    "#     except:\n",
    "#         first_pass = \"\"\n",
    "#     try:\n",
    "#         solution_words = \";\".join(re.findall(regex_solution_word, ex))\n",
    "#     except:\n",
    "#         solution_words = \"\"\n",
    "#     try:\n",
    "#         solution = re.findall(regex_solution, ex)[0]\n",
    "#     except:\n",
    "#         solution = \"\"\n",
    "#     return {\n",
    "#         \"idx\": ex_idx,\n",
    "#         \"word_guesses\": word_guesses,\n",
    "#         \"first_pass\": first_pass,\n",
    "#         \"solution_words\": solution_words,\n",
    "#         \"solution\": solution,\n",
    "#     }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Create gold parsed outputs\n",
    "\n",
    "# results = []\n",
    "# for ex_idx, ex in tqdm(enumerate(eval_dataset), total=len(eval_dataset)):\n",
    "#     gold_output = ex[\"conversations\"][1][\"value\"]\n",
    "#     parsed_output = parse_generation(ex_idx, gold_output)\n",
    "#     results.append(parsed_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44de2a7c-fffc-4422-aa6e-b2c5a2a68474",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# if model_type == \"llama\":\n",
    "#     tokenizer.padding_side = \"right\"\n",
    "# elif model_type in (\"phi-3\", \"gemma\"):\n",
    "#     tokenizer.padding_side = \"left\"\n",
    "\n",
    "# results = []\n",
    "# batch_size = 134\n",
    "# for i in tqdm(range(0, len(eval_dataset), batch_size), total=len(eval_dataset)//batch_size):\n",
    "#     batch = eval_dataset[i:i+batch_size]\n",
    "#     print(batch['conversations'][0])\n",
    "#     if model_type == \"llama\":\n",
    "#         input = [[{\"role\": \"user\", \"content\": example[0][\"value\"]}] for example in batch[\"conversations\"]]\n",
    "    # elif model_type == \"phi-3\":\n",
    "        # print(batch['conversation'])\n",
    "        # input = [[item[0]] for item in batch[\"conversations\"]]\n",
    "\n",
    "    # inputs = tokenizer.apply_chat_template(\n",
    "    #     input,\n",
    "    #     tokenize = True,\n",
    "    #     add_generation_prompt = True, # Must add for generation\n",
    "    #     return_tensors = \"pt\",\n",
    "    #     padding=True,\n",
    "    #     truncation=True,\n",
    "    #     return_dict=True\n",
    "    # )\n",
    "    # inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "    # outputs = model.generate(**inputs, max_new_tokens = 500, use_cache = True, eos_token_id = stop_token_id)\n",
    "\n",
    "    # model_generations = tokenizer.batch_decode(outputs)\n",
    "    # for ex_idx, ex in enumerate(model_generations):\n",
    "    #     out_dic = parse_generation(ex_idx + i, ex)\n",
    "    #     if i == 0 and ex_idx <= 5:\n",
    "    #         print(ex)\n",
    "    #         print(out_dic)\n",
    "    #     results.append(out_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfe87f7-c6b5-40fe-b713-5182c496a2ec",
   "metadata": {},
   "source": [
    "# qui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12710559-188b-4176-b03b-293281359f67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word_guesses': 'pire;cent',\n",
       " 'first_pass': 'TEM pire cent I',\n",
       " 'solution_words': 'Tempi;recenti',\n",
       " 'solution': 'Tempi recenti'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Single-example generation\n",
    "\n",
    "stop_token_id = model.config.eos_token_id\n",
    "if model_type == \"gemma\":\n",
    "    stop_token = \"<|eot_id|>\"\n",
    "    stop_token_id = tokenizer.encode(stop_token)[0]\n",
    "\n",
    "ex_idx = 5\n",
    "example = eval_dataset[ex_idx][\"conversations\"][0]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": example[\"value\"]}\n",
    "    ],\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors = \"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    ")\n",
    "inputs = inputs.to('cuda:0')\n",
    "outputs = model.generate(input_ids = inputs, max_new_tokens = 500, use_cache = True, eos_token_id = stop_token_id)\n",
    "model_generations = tokenizer.batch_decode(outputs)\n",
    "parse_generation(model_generations[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c90a90b-9ee2-4c92-b619-85ce7fcb1988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1998"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa83bbd7-4c7a-4e1c-926b-58053ec98e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversations'],\n",
       "    num_rows: 1998\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b414b50d-24b0-4728-93fe-70e4c6514b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['conversations'],\n",
      "    num_rows: 10\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Seleziona un subset di dimensione 10\n",
    "subset = eval_dataset.select(range(10))\n",
    "\n",
    "# Verifica il subset\n",
    "print(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d464efb6-5169-4734-9457-5dda3ef338a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                        | 1/10 [00:06<00:58,  6.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><|user|> Risolvi gli indizi tra parentesi per ottenere una prima lettura, e usa la chiave di lettura per ottenere la soluzione del rebus.\n",
      "\n",
      "Rebus: [Sono pari nell' odio] C CI [Altro nome dei gicheri] NF [Se scappa, va in esilio] SC [Le ha l'armadio]\n",
      "Chiave di lettura: 6 12<|end|><|assistant|> Procediamo alla risoluzione del rebus passo per passo:\n",
      "- [Sono pari nell' odio] = do\n",
      "- C C I = C C I\n",
      "- [Altro nome dei gicheri] = ari\n",
      "- N F = N F\n",
      "- [Se scappa, va in esilio] = re\n",
      "- S C = S C\n",
      "- [Le ha l'armadio] = ante\n",
      "\n",
      "Prima lettura: do C CI ari NF re SC ante\n",
      "\n",
      "Ora componiamo la soluzione seguendo la chiave risolutiva:\n",
      "6 = Doccia\n",
      "12 = rinfrescante\n",
      "\n",
      "Soluzione: Doccia rinfrescante\n",
      "<|end|><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for ex_idx in tqdm(range(len(subset)), total=len(subset)):\n",
    "    # Ottieni il primo esempio di conversazione (senza batch)\n",
    "    examples = subset[ex_idx]\n",
    "    prompts = examples['conversations'][0]\n",
    "\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": prompts[\"value\"]}\n",
    "    ],\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors = \"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    )\n",
    "    inputs = inputs.to('cuda:0')\n",
    "    \n",
    "    # print(example)  # Puoi usare questo per controllare il contenuto dell'esempio\n",
    "\n",
    "    # input = [[item[0]] for item in examples]\n",
    "    \n",
    "    # # A questo punto puoi fare inferenza su `input`, ad esempio:\n",
    "    # inputs = tokenizer.apply_chat_template(\n",
    "    #     input,\n",
    "    #     tokenize=True,\n",
    "    #     add_generation_prompt=True,  # Aggiungi il prompt per la generazione\n",
    "    #     return_tensors=\"pt\",\n",
    "    #     padding=True,\n",
    "    #     truncation=True,\n",
    "    #     return_dict=True\n",
    "    # )\n",
    "\n",
    "    # # Inferenza sul modello\n",
    "    outputs = model.generate(inputs, max_new_tokens=500, use_cache=True, eos_token_id=stop_token_id)\n",
    "    model_generations = tokenizer.batch_decode(outputs)\n",
    "    \n",
    "    # # Aggiungi il risultato alla lista dei risultati\n",
    "    results.append(parse_generation(model_generations[0]))\n",
    "    \n",
    "    # # Opzionalmente, puoi fare qualcosa con i risultati, come stamparli:\n",
    "    print(model_generations[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb0968a-c44a-46e7-9ec6-58af74e79c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742a8d4b-d2ee-4967-88f2-d76f0498dd2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
