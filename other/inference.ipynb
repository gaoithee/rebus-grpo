{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16177c77-1367-4623-83a9-e42780e06ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.739 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 7.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'saracandu/SmolGRPO-135M'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'saracandu/SmolGRPO-135M' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m load_in_4bit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;66;03m# Use 4bit quantization to reduce memory usage. Can be False.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m model_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphi\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# llama, phi-3, gemma\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mFastLanguageModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msaracandu/SmolGRPO-135M\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# MODEL OR ADAPTER FOLDER\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m FastLanguageModel\u001b[38;5;241m.\u001b[39mfor_inference(model)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/unsloth/models/loader.py:363\u001b[0m, in \u001b[0;36mFastLanguageModel.from_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, full_finetuning, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, *args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mdispatch_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_get_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_patcher\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdispatch_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_peft\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfast_inference\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfast_inference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfloat8_kv_cache\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfloat8_kv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_lora_rank\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_lora_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resize_model_vocab \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m     model\u001b[38;5;241m.\u001b[39mresize_token_embeddings(resize_model_vocab)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/unsloth/models/llama.py:1833\u001b[0m, in \u001b[0;36mFastLlamaModel.from_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, **kwargs)\u001b[0m\n\u001b[1;32m   1831\u001b[0m \u001b[38;5;66;03m# Counteract saved tokenizers\u001b[39;00m\n\u001b[1;32m   1832\u001b[0m tokenizer_name \u001b[38;5;241m=\u001b[39m model_name \u001b[38;5;28;01mif\u001b[39;00m tokenizer_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m tokenizer_name\n\u001b[0;32m-> 1833\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload_correct_tokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1834\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1835\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_max_length\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1836\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mright\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1840\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1842\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m patch_tokenizer(model, tokenizer)\n\u001b[1;32m   1843\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m model_patcher\u001b[38;5;241m.\u001b[39mpost_patch(model, tokenizer)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/unsloth/tokenizer_utils.py:561\u001b[0m, in \u001b[0;36mload_correct_tokenizer\u001b[0;34m(tokenizer_name, model_max_length, padding_side, token, trust_remote_code, cache_dir, fix_tokenizer)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_correct_tokenizer\u001b[39m(\n\u001b[1;32m    553\u001b[0m     tokenizer_name,\n\u001b[1;32m    554\u001b[0m     model_max_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    559\u001b[0m     fix_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    560\u001b[0m ):\n\u001b[0;32m--> 561\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_load_correct_tokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_max_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_max_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;66;03m### 1. Fixup tokenizer's chat_template\u001b[39;00m\n\u001b[1;32m    572\u001b[0m     old_chat_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(tokenizer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_template\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/unsloth/tokenizer_utils.py:516\u001b[0m, in \u001b[0;36m_load_correct_tokenizer\u001b[0;34m(tokenizer_name, model_max_length, padding_side, token, trust_remote_code, cache_dir, fix_tokenizer)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;66;03m# Unsure why this occurs!\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(slow_tokenizer) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mbool\u001b[39m: slow_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m fast_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_max_length\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_max_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fix_tokenizer \u001b[38;5;129;01mor\u001b[39;00m tokenizer_name \u001b[38;5;129;01min\u001b[39;00m IGNORED_TOKENIZER_NAMES:\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fast_tokenizer\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:1028\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1025\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[38;5;241m=\u001b[39m TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 1028\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class_fast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1030\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2046\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;66;03m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\u001b[39;00m\n\u001b[1;32m   2044\u001b[0m \u001b[38;5;66;03m# loaded directly from the GGUF file.\u001b[39;00m\n\u001b[1;32m   2045\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gguf_file:\n\u001b[0;32m-> 2046\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   2047\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2048\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2049\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2050\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2051\u001b[0m     )\n\u001b[1;32m   2053\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2054\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'saracandu/SmolGRPO-135M'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'saracandu/SmolGRPO-135M' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer."
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 1248 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model_type = \"phi\" # llama, phi-3, gemma\n",
    "\n",
    "model = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"saracandu/SmolGRPO-135M\", # MODEL OR ADAPTER FOLDER\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08c24cd-f69f-4a07-827a-a924cd7e4dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "eval_dataset = load_dataset('saracandu/eureka-rebus-grpo', data_files = ['id_test.csv'], split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c270e9-4145-40f2-ba84-45f906585938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single-example generation\n",
    "\n",
    "stop_token_id = model.config.eos_token_id\n",
    "if model_type == \"gemma\":\n",
    "    stop_token = \"<|eot_id|>\"\n",
    "    stop_token_id = tokenizer.encode(stop_token)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63b6c09-15fd-4769-be32-41f05117d38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "if model_type == \"llama\":\n",
    "    tokenizer.padding_side = \"right\"\n",
    "elif model_type in (\"phi-3\", \"gemma\"):\n",
    "    tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44647cf2-fb92-422d-8dac-df5abb094ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "regex_word_guess = '- \\[.* = (.*)'\n",
    "regex_firstpass = 'Prima lettura: (.*)'\n",
    "regex_solution_word = \"\\d+ = (.*)\"\n",
    "regex_solution = \"Soluzione: (.*)\"\n",
    "\n",
    "def parse_generation(data):\n",
    "    try:\n",
    "        word_guesses = \";\".join(re.findall(regex_word_guess, data))\n",
    "    except:\n",
    "        word_guesses = \"\"\n",
    "    try:\n",
    "        first_pass = re.findall(regex_firstpass, data)[0]\n",
    "    except:\n",
    "        first_pass = \"\"\n",
    "    try:\n",
    "        solution_words = \";\".join(re.findall(regex_solution_word, data))\n",
    "    except:\n",
    "        solution_words = \"\"\n",
    "    try:\n",
    "        solution = re.findall(regex_solution, data)[0]\n",
    "    except:\n",
    "        solution = \"\"\n",
    "    \n",
    "    return {\n",
    "        \"word_guesses\": word_guesses,\n",
    "        \"first_pass\": first_pass,\n",
    "        \"solution_words\": solution_words,\n",
    "        \"solution\": solution,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb4fe95-aab3-4d38-9b65-cb28e8c9851d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_dataset['conversations'][5][0]['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173522d1-def2-4359-98f0-66a9a67241bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse_generation(eval_dataset['conversations'][5][1]['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94f36f2-ea7c-4044-acfc-f1fe407db6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = tokenizer(eval_dataset['conversations'][5][0]['value'], return_tensors=\"pt\")[\"input_ids\"].to('cuda:0')\n",
    "# outputs = model.generate(input_ids = inputs, max_new_tokens = 500, use_cache = True, eos_token_id = stop_token_id)\n",
    "# model_generations = tokenizer.batch_decode(outputs)\n",
    "# print(model_generations[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6cd5cd-2cc3-4cc1-b921-9a93410f0d83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from unsloth import FastLanguageModel\n",
    "\n",
    "# max_seq_length = 1248 # Choose any! We auto support RoPE Scaling internally!\n",
    "# dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "# load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# model_type = \"gemma\" # llama, phi-3, gemma\n",
    "\n",
    "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#     model_name = \"gsarti/gemma-2-2b-rebus-solver-fp16\", # MODEL OR ADAPTER FOLDER\n",
    "#     max_seq_length = max_seq_length,\n",
    "#     dtype = dtype,\n",
    "#     load_in_4bit = load_in_4bit,\n",
    "# )\n",
    "# FastLanguageModel.for_inference(model)\n",
    "\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# eval_dataset = load_dataset('gsarti/eureka-rebus', 'llm_sft', data_files=[\"id_test.jsonl\", \"ood_test.jsonl\"], split = \"train\")\n",
    "\n",
    "# # Single-example generation\n",
    "\n",
    "# stop_token_id = model.config.eos_token_id\n",
    "# if model_type == \"gemma\":\n",
    "#     stop_token = \"<|eot_id|>\"\n",
    "#     stop_token_id = tokenizer.encode(stop_token)[0]\n",
    "\n",
    "# ex_idx = 5\n",
    "# example = eval_dataset[ex_idx][\"conversations\"][0]\n",
    "# inputs = tokenizer.apply_chat_template(\n",
    "#     [\n",
    "#         {\"role\": \"user\", \"content\": example[\"value\"]}\n",
    "#     ],\n",
    "#     add_generation_prompt=True,\n",
    "#     return_tensors = \"pt\",\n",
    "#     padding=True,\n",
    "#     truncation=True,\n",
    "# )\n",
    "# inputs = inputs.to('cuda:0')\n",
    "# outputs = model.generate(input_ids = inputs, max_new_tokens = 500, use_cache = True, eos_token_id = stop_token_id)\n",
    "# model_generations = tokenizer.batch_decode(outputs)\n",
    "# print(model_generations[0])\n",
    "\n",
    "\n",
    "\n",
    "# import re\n",
    "\n",
    "# regex_word_guess = '- \\[.* = (.*)'\n",
    "# regex_firstpass = 'Prima lettura: (.*)'\n",
    "# regex_solution_word = \"\\d+ = (.*)\"\n",
    "# regex_solution = \"Soluzione: (.*)\"\n",
    "\n",
    "# def parse_generation(ex_idx, ex):\n",
    "#     try:\n",
    "#         word_guesses = \";\".join(re.findall(regex_word_guess, ex))\n",
    "#     except:\n",
    "#         word_guesses = \"\"\n",
    "#     try:\n",
    "#         first_pass = re.findall(regex_firstpass, ex)[0]\n",
    "#     except:\n",
    "#         first_pass = \"\"\n",
    "#     try:\n",
    "#         solution_words = \";\".join(re.findall(regex_solution_word, ex))\n",
    "#     except:\n",
    "#         solution_words = \"\"\n",
    "#     try:\n",
    "#         solution = re.findall(regex_solution, ex)[0]\n",
    "#     except:\n",
    "#         solution = \"\"\n",
    "#     return {\n",
    "#         \"idx\": ex_idx,\n",
    "#         \"word_guesses\": word_guesses,\n",
    "#         \"first_pass\": first_pass,\n",
    "#         \"solution_words\": solution_words,\n",
    "#         \"solution\": solution,\n",
    "#     }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Create gold parsed outputs\n",
    "\n",
    "# results = []\n",
    "# for ex_idx, ex in tqdm(enumerate(eval_dataset), total=len(eval_dataset)):\n",
    "#     gold_output = ex[\"conversations\"][1][\"value\"]\n",
    "#     parsed_output = parse_generation(ex_idx, gold_output)\n",
    "#     results.append(parsed_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44de2a7c-fffc-4422-aa6e-b2c5a2a68474",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# if model_type == \"llama\":\n",
    "#     tokenizer.padding_side = \"right\"\n",
    "# elif model_type in (\"phi-3\", \"gemma\"):\n",
    "#     tokenizer.padding_side = \"left\"\n",
    "\n",
    "# results = []\n",
    "# batch_size = 134\n",
    "# for i in tqdm(range(0, len(eval_dataset), batch_size), total=len(eval_dataset)//batch_size):\n",
    "#     batch = eval_dataset[i:i+batch_size]\n",
    "#     print(batch['conversations'][0])\n",
    "#     if model_type == \"llama\":\n",
    "#         input = [[{\"role\": \"user\", \"content\": example[0][\"value\"]}] for example in batch[\"conversations\"]]\n",
    "    # elif model_type == \"phi-3\":\n",
    "        # print(batch['conversation'])\n",
    "        # input = [[item[0]] for item in batch[\"conversations\"]]\n",
    "\n",
    "    # inputs = tokenizer.apply_chat_template(\n",
    "    #     input,\n",
    "    #     tokenize = True,\n",
    "    #     add_generation_prompt = True, # Must add for generation\n",
    "    #     return_tensors = \"pt\",\n",
    "    #     padding=True,\n",
    "    #     truncation=True,\n",
    "    #     return_dict=True\n",
    "    # )\n",
    "    # inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "    # outputs = model.generate(**inputs, max_new_tokens = 500, use_cache = True, eos_token_id = stop_token_id)\n",
    "\n",
    "    # model_generations = tokenizer.batch_decode(outputs)\n",
    "    # for ex_idx, ex in enumerate(model_generations):\n",
    "    #     out_dic = parse_generation(ex_idx + i, ex)\n",
    "    #     if i == 0 and ex_idx <= 5:\n",
    "    #         print(ex)\n",
    "    #         print(out_dic)\n",
    "    #     results.append(out_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfe87f7-c6b5-40fe-b713-5182c496a2ec",
   "metadata": {},
   "source": [
    "# qui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12710559-188b-4176-b03b-293281359f67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Single-example generation\n",
    "\n",
    "stop_token_id = model.config.eos_token_id\n",
    "if model_type == \"gemma\":\n",
    "    stop_token = \"<|eot_id|>\"\n",
    "    stop_token_id = tokenizer.encode(stop_token)[0]\n",
    "\n",
    "ex_idx = 5\n",
    "example = eval_dataset[ex_idx]\n",
    "inputs = tokenizer(\n",
    "    example['prompt'],\n",
    "    return_tensors = \"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    ")\n",
    "inputs = inputs.to('cuda:0')\n",
    "outputs = model.generate(input_ids = inputs[\"input_ids\"], max_new_tokens = 500, use_cache = True, eos_token_id = stop_token_id)\n",
    "model_generations = tokenizer.batch_decode(outputs)\n",
    "parse_generation(model_generations[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c90a90b-9ee2-4c92-b619-85ce7fcb1988",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa83bbd7-4c7a-4e1c-926b-58053ec98e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b414b50d-24b0-4728-93fe-70e4c6514b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleziona un subset di dimensione 10\n",
    "subset = eval_dataset.select(range(10))\n",
    "\n",
    "# Verifica il subset\n",
    "print(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d464efb6-5169-4734-9457-5dda3ef338a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for ex_idx in tqdm(range(len(subset)), total=len(subset)):\n",
    "    # Ottieni il primo esempio di conversazione (senza batch)\n",
    "    examples = subset[ex_idx]\n",
    "    prompts = examples['conversations'][0]\n",
    "\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": prompts[\"value\"]}\n",
    "    ],\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors = \"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    )\n",
    "    inputs = inputs.to('cuda:0')\n",
    "    \n",
    "    # print(example)  # Puoi usare questo per controllare il contenuto dell'esempio\n",
    "\n",
    "    # input = [[item[0]] for item in examples]\n",
    "    \n",
    "    # # A questo punto puoi fare inferenza su `input`, ad esempio:\n",
    "    # inputs = tokenizer.apply_chat_template(\n",
    "    #     input,\n",
    "    #     tokenize=True,\n",
    "    #     add_generation_prompt=True,  # Aggiungi il prompt per la generazione\n",
    "    #     return_tensors=\"pt\",\n",
    "    #     padding=True,\n",
    "    #     truncation=True,\n",
    "    #     return_dict=True\n",
    "    # )\n",
    "\n",
    "    # # Inferenza sul modello\n",
    "    outputs = model.generate(inputs, max_new_tokens=500, use_cache=True, eos_token_id=stop_token_id)\n",
    "    model_generations = tokenizer.batch_decode(outputs)\n",
    "    \n",
    "    # # Aggiungi il risultato alla lista dei risultati\n",
    "    results.append(parse_generation(model_generations[0]))\n",
    "    \n",
    "    # # Opzionalmente, puoi fare qualcosa con i risultati, come stamparli:\n",
    "    print(model_generations[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb0968a-c44a-46e7-9ec6-58af74e79c0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742a8d4b-d2ee-4967-88f2-d76f0498dd2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c28e27-a57d-418f-bb4c-3861bbc29d36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
